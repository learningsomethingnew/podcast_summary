{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learningsomethingnew/podcast_summary/blob/main/summarize_podcasts_rough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrHN5XDeRkXL"
      },
      "source": [
        "# Podcast Summarizer\n",
        "Podcast are a wealth of information, but they tend to have tons of episodes that are upwards of 1-2 hours per. This Google Colab will take a given podcast and will create a csv document for each audio/mp3 audio episode for free! There is also an option to use OpenAI's GPT3, which does charge, but is super cheap. \n",
        "\n",
        "## Details\n",
        "Each episode will get a CSV in the output folder that will contain:\n",
        "\n",
        "*   Speaker identification (optional)\n",
        "*   Using Speach to Text transcribe what was said by each speaker. Example: \n",
        "  > SPEAKER_27: Hey, Molly Webster here. Last week, we had a story in collaboration with NPR's rough translation about an amateur network of strangers trying to get abortion pills into Ukraine in the early months of the war. Part two, where we go into Ukraine, that's coming out next week. In the meantime, I've got a little Radiolab rewind for you. I wanted to play you a story that we did in 2015, so eight years ago. Because I've been thinking about it a lot while I've been working on the Ukraine piece. It is also a mix of border crossings and ethics and medical questions and pregnancy and crisis and it just has a really big heart. It's called Birth Story. Here it is.  <br>\n",
        "SPEAKER_11: Alright. <br>\n",
        "SPEAKER_13: You're listening to Radiolab.... \n",
        "*   Summerize the episode\n",
        "  > In October, reporter Katz Laszlo and I landed in the Ukrainian city of Lviv. We came here because of a donation of pills fueled by one story of war. Pills meant to offer some relief and maybe restore some choice. But in Ukraine, we'd hear so many different stories about the ways that people were interacting with these pills in a war. It made us rethink our understanding of how we talk about these pills and the way we talk about choice....\n",
        "\n",
        "## How long does this need to run?\n",
        "* In testing, for a 1 hour podcast episode it takes around 25 minutes.\n",
        "\n",
        "## Which Runtime Type?\n",
        "You will need to use a **GPU** Runtime. In order to select a runtime, on the menu: \n",
        "1. Click “Runtime” -> “Change runtime type”\n",
        "2. Under \"Hardware Accelerator\" -> select \"GPU\" \n",
        "3. Click “Save”\n",
        "\n",
        "## How do I run this?\n",
        "1. Go to the Required Information section, below, and fill out the forms.\n",
        "2. Once filled in, on the menu, click “Runtime” -> “Run All”\n",
        " * Or you can press \"shift + enter\" on your keyboard until you get to the end of this document to run each cell.\n",
        "\n",
        "## Wait, the runtime shutdown before completing\n",
        "No need to worry. This code has been setup to pick up where it left off in processing. Just reconnect, with GPU collab. Depending on how long each episode is and how many episodes, this can take a few sessions. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUkUCdROhEl9"
      },
      "source": [
        "# Required Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAZ-NuQBz33c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "364e6cbf-bf35-42d7-8e48-11f033e4ce62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-09 07:40:32,283 - Verifying the current runtime environment has a GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-09 07:40:32,996 - Runtime environment has a GPU! Now installing packages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.1 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.1 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m71.7/81.1 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 KB\u001b[0m \u001b[31m609.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff\n",
            "Successfully installed backoff-2.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-09 07:41:21,361 - All packages have been installed\n",
            "2023-02-09 07:41:21,365 - Importing the packages\n",
            "2023-02-09 07:41:24,012 - Generating grammar tables from /usr/lib/python3.8/lib2to3/Grammar.txt\n",
            "2023-02-09 07:41:24,057 - Generating grammar tables from /usr/lib/python3.8/lib2to3/PatternGrammar.txt\n",
            "2023-02-09 07:41:27,221 - NumExpr defaulting to 2 threads.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "2023-02-09 07:41:29,480 - Loading functions\n",
            "2023-02-09 07:41:29,485 - Creating directory '/content/drive/MyDrive/Summarize_Podcasts/ml_models/' in Google Drive\n",
            "2023-02-09 07:41:29,492 - Grabbing the latest podcast xml feed\n",
            "2023-02-09 07:41:30,806 - There are 150 episodes in this podcast\n",
            "2023-02-09 07:41:30,812 - Making sure that the needed directories that we need exist and/or creating as needed\n",
            "2023-02-09 07:41:30,814 - Creating directory '/content/drive/MyDrive/Summarize_Podcasts/radiolab/full_mp3/' in Google Drive\n",
            "2023-02-09 07:41:30,823 - Creating directory '/content/drive/MyDrive/Summarize_Podcasts/radiolab/full_wav/' in Google Drive\n",
            "2023-02-09 07:41:30,828 - Creating directory '/content/drive/MyDrive/Summarize_Podcasts/radiolab/full_mp3/split_mp3s/' in Google Drive\n",
            "2023-02-09 07:41:30,833 - Creating directory '/content/drive/MyDrive/Summarize_Podcasts/radiolab/transcripts/' in Google Drive\n",
            "2023-02-09 07:41:30,843 - Saving podcast episodes for tracking at /content/drive/MyDrive/Summarize_Podcasts/radiolab/radiolab_podcast_feed.json\n",
            "2023-02-09 07:41:30,844 - You have selected to download 1 episodes\n",
            "2023-02-09 07:41:30,849 - Starting on Ukraine: The Handoff\n",
            "2023-02-09 07:41:30,850 - Downloading Ukraine: The Handoff\n",
            "2023-02-09 07:41:32,495 - Downloaded ukraine:_the_handoff.mp3\n",
            "2023-02-09 07:41:32,502 - Starting transcription. 1hr episode takes around 20-35 min.\n",
            "2023-02-09 07:50:45,950 - Transcription has completed in 553.446123123169 seconds\n",
            "2023-02-09 07:50:45,955 - Converting the output to template\n",
            "2023-02-09 07:50:46,005 - You can find this file at /content/drive/MyDrive/Summarize_Podcasts/radiolab/transcripts/ukraine:_the_handoff_full_speaker_transcript.txt\n",
            "2023-02-09 07:50:46,013 - Breaking up the transcript into smaller chunks for summarizing\n",
            "2023-02-09 07:51:22,662 - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
            "2023-02-09 07:51:22,665 - Backing off completions_with_backoff(...) for 0.9s (openai.error.RateLimitError: The server had an error while processing your request. Sorry about that!)\n",
            "2023-02-09 07:51:25,371 - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
            "2023-02-09 07:51:25,372 - Backing off completions_with_backoff(...) for 1.0s (openai.error.RateLimitError: The server had an error while processing your request. Sorry about that!)\n",
            "2023-02-09 07:52:16,673 - Saving the summary at /content/drive/MyDrive/Summarize_Podcasts/radiolab/transcripts/ukraine:_the_handoff_summary.txt\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "#@title Please fill out this form\n",
        "\n",
        "#@markdown ## Podcast XML <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/256px-Feed-icon.svg.png\" alt=\"RSS Feed Icon\" height=35; />\n",
        "#@markdown Please provide the Podcast's RSS Feed XML link here.\n",
        "\n",
        "podcast_xml = \"http://feeds.feedburner.com/radiolab\" #@param {type:\"string\"}\n",
        "# openai.api_key = apikey_for_openai\n",
        "#@markdown Number of episodes you would like to summarize, starting from most recent? Use 0 for all episodes.\n",
        "num_of_episodes = 1 #@param {type:\"integer\"}\n",
        "#@markdown <br>\n",
        "\n",
        "\n",
        "\n",
        "#@markdown ### Hugging Face Setup <img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" alt=\"RSS Feed Icon\" height=35; />\n",
        "#@markdown Do you want to get an output file that identifies each speaker? \n",
        "#@markdown If \"False\" then you can skip the api key\n",
        "identify_speakers = \"False\" #@param [\"True\", \"False\"]\n",
        "identify_speakers = identify_speakers.lower()==\"true\"\n",
        "#@markdown 1. Please provide a Hugging Face api key. [You can get one by following this guide](https://huggingface.co/docs/hub/security-tokens)\n",
        "apikey_for_hugging_face = '' #@param {type:\"string\"}\n",
        "#@markdown 2. Visit [hf.co/pyannote/speaker-diarization](https://hf.co/pyannote/speaker-diarization) and and accept user conditions <br> <img src=\"https://i.imgur.com/HpwAGpR.png\" alt=\"RSS Feed Icon\" height=120; />\n",
        "#@markdown 3. Visit [hf.co/pyannote/segmentation](hf.co/pyannote/segmentation) and accept user conditions\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown  ## Summarize Setup <img src=\"https://i.imgur.com/Mc95bds.png\" alt=\"RSS Feed Icon\" height=40; />\n",
        "\n",
        "#@markdown In order to summarize a podcast episode, you will need to select which tool you would like to use. This colab is setup with:<br>\n",
        "#@markdown * Sumy - A free library that uses Latent Semantic Analysis (LSA)... tldr: It is OK and summarizes by finding high value sentences \n",
        "#@markdown and combining them.\n",
        "#@markdown * GPT3 - Requires an [API key and a credit card after the free credits](https://elephas.app/blog/how-to-create-openai-api-keys-cl5c4f21d281431po7k8fgyol0). More advanced & powerful than Sumy.\n",
        "#@markdown   *  In testing, 3, 1 hour podcasts summarized cost me ~1 dollar USD. \n",
        "summarization_tool = \"GPT3 (Requires API Key)\" #@param [\"Sumy (Free)\", \"GPT3 (Requires API Key)\"]\n",
        "#@markdown if you selected GPT3, you will need to input the API key here. Otherwise you can leave blank.\n",
        "apikey_for_openai = \"\" #@param {type:\"string\"}\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ## Optional Settings\n",
        "#@markdown Do you want to summarize each episode?\n",
        "summarize_episode = \"True\" #@param [\"True\", \"False\"]\n",
        "\n",
        "summarize_episode = summarize_episode.lower()==\"true\"\n",
        "#@markdown Percent of sentences do you want to trim off the front and back of each\n",
        "#@markdown episode? The idea here is to skip ads, intros, and outros to reduce noise\n",
        "#@markdown for the summary. If 800 sentences, at 10%, 80 sentences from the front\n",
        "#@markdown  and back will be ignored for summarizing\n",
        "percent_sentences_to_skip = 8 #@param {type:\"slider\", min:0, max:40, step:1}\n",
        "\n",
        "#@markdown ### Google Drive <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Google_Drive_icon_%282020%29.svg/2295px-Google_Drive_icon_%282020%29.svg.png\" alt=\"RSS Feed Icon\" height=20; />\n",
        "#@markdown Free Colab has a limited runtime. By leveraging your Google Drive, we can iterate through the podcast \n",
        "#@markdown and pick back up where we left off after the runtime ends. \n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "#@markdown Google Drive Directory Path. (Can leave as default)\n",
        "path_to_use = \"/content/drive/MyDrive/Summarize_Podcasts\" #@param {type:\"string\"}\n",
        "#@markdown ### Whisper (Speech To Text)\n",
        "#@markdown Which speech to text model would you like to use. The larger the model, \n",
        "#@markdown the more RAM is needed. Only change if you know about what this means\n",
        "whisper_model = \"medium.en\" #@param [\"large-v2\", \"large-v1\", \"large\", \"medium\", \"medium.en\", \"base\", \"base.en\", \"tiny\", \"tiny.en\", \"\"]\n",
        "#@markdown If you want Colab to auto disconnect the environment when completed\n",
        "auto_shutoff = \"True\" #@param [\"True\", \"False\"]\n",
        "auto_shutoff = auto_shutoff.lower()==\"true\"\n",
        "\n",
        "\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "# logging setup\n",
        "logger = logging.getLogger('podcast_summary')\n",
        "\n",
        "logging.basicConfig(\n",
        "  format='%(asctime)s - %(message)s', \n",
        "  level=logging.INFO,\n",
        "  force=True\n",
        ")\n",
        "\n",
        "# validate that we have the right runtime environment\n",
        "logger.info(\"Verifying the current runtime environment has a GPU\")\n",
        "result = subprocess.run([\"nvidia-smi\", \"-L\"], stdout=subprocess.PIPE)\n",
        "output = result.stdout.decode('utf-8')\n",
        "# catch the runtime if it does not have a GPU. Example output \"GPU 0: Tesla T4\"\n",
        "if 'gpu' not in output.lower():\n",
        "  logger.error(\"No GPU detected\")\n",
        "  raise SystemExit(\"ERROR: !!!!! Please change the Runtime environment to include a GPU. See the instructions above.\")\n",
        "else: \n",
        "  logger.info(\"Runtime environment has a GPU! Now installing packages\")\n",
        "\n",
        "# if the packages have already been installed on this run. 'installed', set\n",
        "# in the cell above, will prevent from reinstalling if this cell is reran\n",
        "\n",
        "\n",
        "if identify_speakers:\n",
        "  logger.info(\"Identifying Speakers set to True. Installing needed libraries\")\n",
        "  # pytorch setup\n",
        "  !pip install -qq torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 torchtext==0.12.0\n",
        "  !pip install -qq speechbrain==0.5.12\n",
        "  # pyannote.audio for figuring out who is speaking & when\n",
        "  !pip install -qq pyannote.audio==2.1.1\n",
        "\n",
        "  # for splitting up the audio files and file conversion\n",
        "  !pip install -qq pydub==0.25.1 \n",
        "\n",
        "\n",
        "# requests for downloading the podcast feed and mp3s\n",
        "!pip install -qq requests\n",
        "\n",
        "# for processing the podcast xml\n",
        "!pip install -qq feedparser\n",
        "\n",
        "# for speach to text\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "\n",
        "# for text summerization\n",
        "if summarization_tool.startswith(\"GPT3\"):\n",
        "  !pip install -qq openai==0.26.4\n",
        "else:\n",
        "  !pip install -qq sumy==0.11.0\n",
        "\n",
        "# rate limiting api calls\n",
        "!pip install backoff\n",
        "\n",
        "logger.info(\"All packages have been installed\")\n",
        "\n",
        "logger.info(\"Importing the packages\")\n",
        "# import all the things\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import gc\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "import google.colab\n",
        "from google.colab import runtime\n",
        "\n",
        "import feedparser\n",
        "import requests\n",
        "\n",
        "import backoff\n",
        "import whisper\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "if identify_speakers=='True':\n",
        "  from pyannote.audio import Audio \n",
        "  from pyannote.audio import Pipeline\n",
        "  from pydub import AudioSegment\n",
        "  from pydub import silence\n",
        "  from pydub.silence import split_on_silence\n",
        "\n",
        "if summarization_tool.startswith(\"GPT3\"):\n",
        "  import openai\n",
        "  openai.api_key = apikey_for_openai\n",
        "else:\n",
        "  from sumy.parsers.plaintext import PlaintextParser\n",
        "  from sumy.nlp.tokenizers import Tokenizer\n",
        "  from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "  from sumy.nlp.stemmers import Stemmer\n",
        "  from sumy.utils import get_stop_words\n",
        "  \n",
        "\n",
        "# max length of the clips. We add .5 seconds to the front and back if we \n",
        "# split on silence as 30 seconds is the max length of whisper. Multiply by 1k\n",
        "# to get milliseconds\n",
        "audio_max_clip_length = 29 * 1000\n",
        "\n",
        "logger.info(\"Loading functions\")\n",
        "#################### FILE HANDLING\n",
        "################################################################################\n",
        "def file_path_validate_get(folder_path: str, file_name: str):\n",
        "  \"\"\"\n",
        "  validates that a path is available for a file. If it is not, then it will create\n",
        "  the path as needed.\n",
        "  save_path: the folder directory\n",
        "  file_name: the name of the file\n",
        "  return: str of file path\n",
        "  \"\"\"\n",
        "  if not os.path.exists(folder_path):\n",
        "      logger.info(f\"Creating directory '{folder_path}' in Google Drive\")\n",
        "      os.makedirs(folder_path)\n",
        "  return os.path.join(folder_path, file_name)\n",
        "\n",
        "def files_in_folder_delete(folder_path: str):\n",
        "  \"\"\"\n",
        "  finds all of the files in the specified folder and then deletes them\n",
        "  folder_path: str path of folder\n",
        "  return: n/a \n",
        "  \"\"\"\n",
        "  for file_name in os.listdir(folder_path):\n",
        "      file_path = os.path.join(folder_path, file_name)\n",
        "      if os.path.isfile(file_path):\n",
        "          os.remove(file_path)\n",
        "\n",
        "def json_save_to(data: dict, file_path: str):\n",
        "  with open(file_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False)\n",
        "\n",
        "def json_load(file_path: str):\n",
        "  with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "\n",
        "#################### ML MODEL LOADING\n",
        "################################################################################\n",
        "# create the path to save the model to\n",
        "path_ml_models = os.path.join(path_to_use, 'ml_models/')\n",
        "_ = file_path_validate_get(path_ml_models, \"\")\n",
        "\n",
        "# download the needed ML models\n",
        "# Diarization is the model needed to identify speakers\n",
        "# only load if we want to identify speakers\n",
        "if identify_speakers:\n",
        "  pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', \n",
        "                                      use_auth_token=apikey_for_hugging_face, \n",
        "                                      #this model doesn't look to see if it is cached\n",
        "                                      # cache_dir=path_ml_models \n",
        "                                      )\n",
        "\n",
        "    # setting up whisper for use with speaker identification\n",
        "  model = whisper.load_model(whisper_model, download_root=path_ml_models)\n",
        "\n",
        "#################### PODCAST DOWNLOADING\n",
        "################################################################################\n",
        "\n",
        "def mp3_download(url: str, file_name: str, save_path: str):\n",
        "  \"\"\"\n",
        "  From a given url, downloads the mp3 into a specified path\n",
        "  \"\"\"\n",
        "  file_path = file_path_validate_get(save_path, file_name)\n",
        "  response = requests.get(url)\n",
        "  open(file_path, 'wb').write(response.content)\n",
        "  logger.info(f\"Downloaded {file_name}\")\n",
        "  del response\n",
        "\n",
        "\n",
        "def podcast_feed_download(rss_feed_url: str):\n",
        "  \"\"\"\n",
        "  downloads the entire podcast's rss, performs a light refactoring\n",
        "  on the file, and then stores feed for later use/reference\n",
        "  \"\"\"\n",
        "  # get the rss_feed\n",
        "  feed = feedparser.parse(rss_feed_url)\n",
        "  logger.info(f\"There are {len(feed.entries)} episodes in this podcast\")\n",
        "  data = {\n",
        "      'title': feed.feed['title'].lower().replace(\" \", \"_\"),\n",
        "      'feed': feed.feed,\n",
        "      'entries': []\n",
        "  }\n",
        "  # keep the data that we want and add some flags for tracking progress\n",
        "  count = 0\n",
        "  for entry in feed.entries:\n",
        "    temp_entry = {}\n",
        "    temp_entry['title'] = entry['title']\n",
        "\n",
        "    # get the href link for the podcast audio file\n",
        "    for link in entry.links:\n",
        "      if link['type'] == 'audio/mpeg':\n",
        "        temp_entry['href'] = link['href']\n",
        "      else:\n",
        "        # ignore the items that don't have an audio href\n",
        "        continue\n",
        "    temp_entry['file_name'] = entry['title'].replace(\" \", \"_\").lower()+\".mp3\"\n",
        "    data['entries'].append(temp_entry)\n",
        "    \n",
        "  del temp_entry\n",
        "  del feed\n",
        "  return data\n",
        "  \n",
        "\n",
        "#################### AUDIO/MP3 MANAGEMENT\n",
        "################################################################################\n",
        "\n",
        "def match_target_amplitude(chunk, target_dBFS):\n",
        "  \"\"\"\n",
        "  Normalize given audio chunk\n",
        "  \"\"\"\n",
        "  return chunk.apply_gain(target_dBFS - chunk.dBFS)\n",
        "\n",
        "def chunk_normalize(chunk):\n",
        "  \"\"\"\n",
        "  Create a silence chunk that's 0.5 seconds (or 500 ms) long for padding\n",
        "  copied from https://stackoverflow.com/a/46001755\n",
        "  \"\"\"\n",
        "  silence_chunk = AudioSegment.silent(duration=500)\n",
        "  # add silence to the begging and end of the chunk\n",
        "  audio_chunk = silence_chunk + chunk + silence_chunk\n",
        "  # normalize the entire chunk\n",
        "  return match_target_amplitude(audio_chunk, -20.0)\n",
        "\n",
        "def chunk_save(chunk, chunk_index):\n",
        "  normalized_chunk = chunk_normalize(chunk)\n",
        "  normalized_chunk.export(path_split_mp3s + f\"chunk_{chunk_index}.mp3\", format=\"mp3\")\n",
        "\n",
        "def mp3_split(audio, start_time, end_time):\n",
        "  # Convert the start and end times from seconds to milliseconds\n",
        "  start_time_ms = start_time * 1000\n",
        "  end_time_ms = end_time * 1000\n",
        "  chunk = audio[start_time_ms:end_time_ms]\n",
        "  return chunk\n",
        "\n",
        "def mp3_convert_to_wav(mp3_file_path: str, wav_file_path: str):\n",
        "    # we need a wave file for us to determine speaker timing of each mp3 file\n",
        "    # check to see if the folders exists\n",
        "    sound = AudioSegment.from_mp3(mp3_file_path)\n",
        "    sound.export(wav_file_path, format=\"wav\")\n",
        "    logger.info(\"MP3 to WAV conversion successful\")\n",
        "    del sound\n",
        "\n",
        "def mp3_split_on_silence(chunk, silence_threshold=-50, min_silence_len=400, clip_duration=30*1000, chunk_index=\"\"):\n",
        "  # split up the audio into chunks\n",
        "  print(f\"Splitting up audio into chunks, chunk_index = {chunk_index}\")\n",
        "  chunks = split_on_silence(chunk, min_silence_len=min_silence_len, silence_thresh=silence_threshold)\n",
        "\n",
        "  for i, chunk in enumerate(chunks):\n",
        "    # identify if any chunks are longer than 29 secs, when saved we pad .5 second front and back\n",
        "    if len(chunk) > 29 * 1000:\n",
        "      mp3_split_on_silence(chunk, min_silence_len=min_silence_len-50, chunk_index=chunk_index+\".\"+str(i))\n",
        "    else:\n",
        "      temp = chunk_index+\".\"+str(i)\n",
        "      chunk_save(chunk, chunk_index+\".\"+str(i))\n",
        "  del chunk\n",
        "  del chunks\n",
        "\n",
        "def split_and_save_thread(audio, start_time, end_time, index):\n",
        "  duration = end_time - start_time\n",
        "  chunk = mp3_split(audio, start_time, end_time)\n",
        "  # if the segment is longer than audio_len_split\n",
        "  if duration > audio_max_clip_length/1000:\n",
        "    print(f\"Main segment has duration of {duration} seconds\")\n",
        "    mp3_split_on_silence(chunk, silence_threshold=-50, min_silence_len=500, clip_duration=30*1000, chunk_index=str(index))\n",
        "  else:\n",
        "    chunk_save(chunk, index)\n",
        "  \n",
        "  del chunk\n",
        "\n",
        "def mp3_split_speaker_segments(audio_segments, mp3_file_path):\n",
        "  num_seg = len(audio_segments)\n",
        "  print(f\"There are {num_seg} segments to split up. This will take a few minutes\")\n",
        "  # Load the entire mp3 file into memory\n",
        "  audio = AudioSegment.from_file(mp3_file_path, format=\"mp3\")\n",
        "\n",
        "  for index, segment in enumerate(audio_segments):\n",
        "    # split up the mp3 into various chunks to make transcribing easier\n",
        "    split_and_save_thread(audio, segment['start'], segment['stop'], index)\n",
        "\n",
        "  del audio\n",
        "\n",
        "\n",
        "#################### SPEAKER IDENTIFICATION/DIARIZATION\n",
        "################################################################################\n",
        "\n",
        "def capture_speaker_changes(diarization):\n",
        "  \"\"\"\n",
        "  Iterates through all of the turns from the diarization and adds up the\n",
        "  time that the same speaker speaks for to create one consistent clip vs smaller\n",
        "  clips.\n",
        "  returns: a list of dicts with keys \"speaker\", \"start\", \"stop\", \"duration\"\n",
        "  \"\"\"\n",
        "  result = []\n",
        "  current_speaker = None\n",
        "  for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "      start, stop = turn.start, turn.end\n",
        "      if current_speaker != speaker:\n",
        "        if current_speaker is not None:\n",
        "            result.append({\n",
        "                \"speaker\": current_speaker, \n",
        "                \"start\": start_time, \n",
        "                \"stop\": stop_time, \n",
        "                \"duration\": stop_time-start_time,\n",
        "                \"transcribed_text\": \"\"})\n",
        "        current_speaker = speaker\n",
        "        start_time = start\n",
        "        stop_time = stop\n",
        "      else:\n",
        "        stop_time = max(stop_time, stop)\n",
        "  if current_speaker is not None:\n",
        "    result.append({\n",
        "      \"speaker\": current_speaker, \n",
        "      \"start\": start_time, \n",
        "      \"stop\": stop_time, \n",
        "      \"duration\": stop_time-start_time,\n",
        "      \"transcribed_text\": \"\"\n",
        "      })\n",
        "  # clear memory of diarization\n",
        "  del diarization\n",
        "  return result\n",
        "\n",
        "def audio_segments_get(wave_file_path):\n",
        "    temp_start = time.time()\n",
        "    WAVE_FILE = {'audio': wave_file_path}\n",
        "    logger.info(\"Identifying speakers. This will take a few minutes\")\n",
        "    waveform, sample_rate = Audio()(WAVE_FILE)\n",
        "    # delete the wave file to save space as we no longer need it\n",
        "    del waveform\n",
        "    return pipeline(WAVE_FILE)\n",
        "    \n",
        "\n",
        "\n",
        "#################### TRANSCRIPTION WITH WHISPER\n",
        "################################################################################\n",
        "## Whisper Transcribe\n",
        "\n",
        "def mp3_transcribe(mp3_path: str):\n",
        "  # load audio and pad/trim it to fit 30 seconds\n",
        "  audio = whisper.load_audio(mp3_path)\n",
        "  audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "  # make log-Mel spectrogram and move to the same device as the model\n",
        "  mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "  # decode the audio\n",
        "  options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
        "  result = whisper.decode(model, mel, options)\n",
        "  del audio\n",
        "  return result.text\n",
        "\n",
        "def mp3_get_index_from_name(chunk_path):\n",
        "  regex= r\"chunk_(\\d+)\"\n",
        "  found = re.findall(regex, chunk_path.split(\"/\")[-1])\n",
        "  return int(found[0])\n",
        "\n",
        "def transcript_get(audio_segments, no_speaker=False):\n",
        "  transcript = \"\"\n",
        "  if no_speaker:\n",
        "      for segment in audio_segments:\n",
        "        transcript += f\"{segment['transcribed_text']} \\n\"\n",
        "  else:\n",
        "    for segment in audio_segments:\n",
        "      transcript += f\"{segment['start']:.2f}-{segment['stop']:.2f} > {segment['speaker']}: {segment['transcribed_text']} \\n\"\n",
        "  return transcript\n",
        "\n",
        "def mp3_chunks_transcribe(audio_segments):\n",
        "  chunks = glob.glob(path_split_mp3s + \"*.mp3\")\n",
        "\n",
        "  for index, chunk_path in enumerate(chunks):\n",
        "    segment_index = mp3_get_index_from_name(chunk_path)\n",
        "\n",
        "    speaker_seg = audio_segments[segment_index]\n",
        "\n",
        "    # by splitting on periods, the len of a regular chunk is 2 and anything\n",
        "    # higher will have a multiple sub parts\n",
        "    file_name_split = chunk_path.split(\"/\")[-1].split(\".\")\n",
        "    # if there are multiple sub parts to one section, we need to append the \n",
        "    # scripts together to complete one chunk. \n",
        "    if len(file_name_split) > 2:\n",
        "      speaker_seg['transcribed_text'] += mp3_transcribe(chunk_path) + \" \"\n",
        "    else:\n",
        "      speaker_seg['transcribed_text'] = mp3_transcribe(chunk_path)\n",
        "\n",
        "    # update the speaker segment\n",
        "    speaker_seg = audio_segments[segment_index]\n",
        "    if index % 25:\n",
        "      logger.info(f\"Transcribed {index} out of {len(chunks)}\")\n",
        "\n",
        "  return audio_segments\n",
        "\n",
        "\n",
        "#################### ML MODEL SUMMARIZATION\n",
        "################################################################################\n",
        "def summy_summarize(text, sentence_count=10):\n",
        "  result = \"\"\n",
        "  # Summarize using sumy LexRank\n",
        "  LANGUAGE = \"english\"\n",
        "  SENTENCES_COUNT= sentence_count\n",
        "  parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
        "  stemmer = Stemmer(LANGUAGE)\n",
        "\n",
        "  summarizer = Summarizer(stemmer)\n",
        "  summarizer.stop_words = get_stop_words(LANGUAGE)\n",
        "\n",
        "  for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
        "    result += str(sentence) + \" \"\n",
        "  # free up mem\n",
        "  del parser\n",
        "  del stemmer\n",
        "  del summarizer\n",
        "  return result\n",
        "\n",
        "def break_up_text(tokens, chunk_size, overlap_size):\n",
        "  if len(tokens) <= chunk_size:\n",
        "    yield tokens\n",
        "  else:\n",
        "    chunk = tokens[:chunk_size]\n",
        "    yield chunk\n",
        "    yield from break_up_text(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
        "\n",
        "def break_up_transcript_to_chunks(text, chunk_size=2000, overlap_size=100):\n",
        "  tokens = word_tokenize(text)\n",
        "  return list(break_up_text(tokens, chunk_size, overlap_size))\n",
        "\n",
        "def convert_to_detokenized_text(tokenized_text):\n",
        "  prompt_text = \" \".join(tokenized_text)\n",
        "  prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
        "  return prompt_text\n",
        "\n",
        "# handle the rate limiting of API calls\n",
        "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
        "def completions_with_backoff(**kwargs):\n",
        "    return openai.Completion.create(**kwargs)\n",
        "\n",
        "def openai_make_summarization(text):\n",
        "  # this was inspired by https://sungkim11.medium.com/how-to-get-around-openai-gpt-3-token-limits-b11583691b32\n",
        "  # split text up by max tokens\n",
        "  logger.info(\"Breaking up the transcript into smaller chunks for summarizing\")\n",
        "  chunks = break_up_transcript_to_chunks(text)\n",
        "  summary = \"\"\n",
        "\n",
        "  for i, chunk in enumerate(chunks):\n",
        "    de_token = convert_to_detokenized_text(chunks[i])\n",
        "    res = completions_with_backoff(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=de_token + \"\\n\\ntl;dr\",\n",
        "        temperature= 0.7,\n",
        "        max_tokens= 1700,  \n",
        "        top_p= 1, \n",
        "        frequency_penalty= 0.0, \n",
        "        presence_penalty= 1\n",
        "      )\n",
        "    \n",
        "    summary += res[\"choices\"][0][\"text\"].strip() + \"\\n\"\n",
        "  return summary\n",
        "  \n",
        "\n",
        "def summarize(transcript):\n",
        "  if summarization_tool.startswith(\"GPT3\"):\n",
        "    return openai_make_summarization(transcript)\n",
        "  else:\n",
        "    return summy_summarize(transcript, sentence_count=10)\n",
        "\n",
        "\n",
        "def transcript_wadsworth_constant(audio_segments):\n",
        "  # cut out the ads and outros for summary\n",
        "  no_speaker = transcript_get(audio_segments, no_speaker=True)\n",
        "  # how many sentences are there\n",
        "  count = len(no_speaker.split(\"\\n\"))\n",
        "  num_skip = math.ceil(count* (percent_sentences_to_skip/100))\n",
        "  temp_speaker = \"\"\n",
        "  for i in no_speaker.split(\"\\n\")[num_skip:-num_skip]:\n",
        "    temp_speaker += i + \"\\n\"\n",
        "  return temp_speaker\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################## MAIN ################################################\n",
        "################################################################################\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "## pull down the podcast\n",
        "logger.info(\"Grabbing the latest podcast xml feed\")\n",
        "feed_data = podcast_feed_download(podcast_xml)\n",
        "\n",
        "\n",
        "#################### PATH CREATION/VALIDATION\n",
        "################################################################################\n",
        "path_working_base_dir = os.path.join(path_to_use, feed_data['title'])\n",
        "path_full_mp3 = os.path.join(path_working_base_dir, \"full_mp3/\")\n",
        "path_full_wave = os.path.join(path_working_base_dir, \"full_wav/\")\n",
        "path_split_mp3s = os.path.join(path_full_mp3, \"split_mp3s/\")\n",
        "path_completed_transcripts = os.path.join(path_working_base_dir, \"transcripts/\")\n",
        "\n",
        "logger.info(\"Making sure that the needed directories that we need exist and/or creating as needed\")\n",
        "_ = file_path_validate_get(path_to_use, \"\")\n",
        "_ = file_path_validate_get(path_full_mp3, \"\")\n",
        "_ = file_path_validate_get(path_full_wave, \"\")\n",
        "_ = file_path_validate_get(path_split_mp3s, \"\")\n",
        "_ = file_path_validate_get(path_completed_transcripts, \"\")\n",
        "\n",
        "## save the podcast feed\n",
        "feed_json_path = path_working_base_dir + f\"/{feed_data['title']}_podcast_feed.json\"\n",
        "json_save_to(feed_data, feed_json_path)\n",
        "logger.info(f\"Saving podcast episodes for tracking at {feed_json_path}\")\n",
        "\n",
        "\n",
        "index = 0\n",
        "entry_duration = []\n",
        "\n",
        "if num_of_episodes==0:\n",
        "  logger.info(\"You have selected all episodes in the podcast. Know that this will probably take multiple sessions to complete.\")\n",
        "  num_of_episodes = len(feed_data['entries'])\n",
        "else:\n",
        "  logger.info(f\"You have selected to download {num_of_episodes} episodes\")\n",
        "# allow for downloading on specific number of files\n",
        "for entry in feed_data['entries'][:num_of_episodes]:\n",
        "  \n",
        "  entry_start=time.time()\n",
        "\n",
        "  logger.info(f\"Starting on {entry['title']}\")\n",
        "  mp3_file_path = path_full_mp3 + entry['file_name']\n",
        "  file_name_no_extension = entry['file_name'][:-4]\n",
        "  file_path_wave = path_full_wave + file_name_no_extension + \".wav\"\n",
        "  file_path_audio_segments = path_full_mp3 + file_name_no_extension + \"_audio_segments.json\"\n",
        "  file_path_transcript_path = path_completed_transcripts + f\"{file_name_no_extension}_full_speaker_transcript.txt\"\n",
        "  file_path_summary = path_completed_transcripts + f\"{file_name_no_extension}_summary.txt\"\n",
        "  \n",
        "  ## DOWNLOAD MP3\n",
        "  # check to see if the mp3 already exists\n",
        "  if os.path.isfile(mp3_file_path) == False:\n",
        "    logger.info(f\"Downloading {entry['title']}\")\n",
        "    mp3_download(url=entry['href'], file_name=entry['file_name'], save_path=path_full_mp3)\n",
        "    \n",
        "\n",
        "  ### DIARIZATION\n",
        "  if identify_speakers == True:\n",
        "    start_diarization = time.time()\n",
        "\n",
        "    #check to see if the segment file already exists\n",
        "    if os.path.isfile(file_path_audio_segments) == False:\n",
        "      # convert to wave file as diarization requires wav\n",
        "      logger.info(\"Converting the mp3 to wav for speaker identification\")\n",
        "      mp3_convert_to_wav(mp3_file_path=mp3_file_path, wav_file_path=file_path_wave)\n",
        "      # get the diarization and then get the output in the format we need it in\n",
        "      audio_segments = capture_speaker_changes(audio_segments_get(file_path_wave))\n",
        "      # save the speaker segments\n",
        "      logger.info(\"Saving the speaker segments\")\n",
        "      json_save_to(audio_segments, file_path_audio_segments)\n",
        "      logger.info(\"Deleting the wav file\")\n",
        "      files_in_folder_delete(path_full_wave)\n",
        "      logger.info(f\"Speaker Identification has completed in {time.time()-start_diarization} seconds\")\n",
        "    # load the speaker segments if \n",
        "    else:\n",
        "      logger.info(\"Found existing speaker segmentation file. Loading...\")\n",
        "      audio_segments = json_load(file_path_audio_segments)\n",
        "\n",
        "    ##### MP3 SPLIT UP & WHISPER Transcribe\n",
        "    if os.path.isfile(file_path_transcript_path) == False:\n",
        "      start_whisper = time.time()\n",
        "      #splitting up the mp3 to smaller chunks to make it easier for whisper &\n",
        "      #requires less ram\n",
        "      logger.info(\"Splitting the MP3 based on speakers\")\n",
        "      start_split = time.time()\n",
        "      mp3_split_speaker_segments(audio_segments, mp3_file_path)\n",
        "      logger.info(f\"Splitting MP3 into chunks has completed in {time.time()-start_split} seconds\")\n",
        "      audio_segments = mp3_chunks_transcribe(audio_segments)\n",
        "      logger.info(f\"Transcription has completed in {time.time()-start_whisper} seconds\")\n",
        "      # save the audio_segments\n",
        "      json_save_to(audio_segments, file_path_audio_segments)\n",
        "      # save the transcript for this episode\n",
        "      with open(file_path_transcript_path, 'w') as f:\n",
        "        f.write(transcript_get(audio_segments))\n",
        "      \n",
        "      ### SUMMARIZE\n",
        "      if os.path.isfile(path_completed_transcripts + f\"/{file_name_no_extension}_summary.txt\") == False and summarize_episode == True:\n",
        "        summary = summarize(transcript_wadsworth_constant(audio_segments))\n",
        "        with open(path_completed_transcripts + f\"/{file_name_no_extension}_summary.txt\", 'w') as f:\n",
        "            f.write(summary)\n",
        "      # delete the split mp3s\n",
        "      logger.info(\"Deleting the split mp3s\")\n",
        "      files_in_folder_delete(\"/content/drive/MyDrive/Summarize_Podcasts/split_mp3s\")\n",
        "  \n",
        "  # user just wants a summary\n",
        "  else:\n",
        "    # transcribe the file if the file doesn't already exist\n",
        "    if os.path.isfile(file_path_transcript_path) == False:\n",
        "      logger.info(\"Starting transcription. 1hr episode takes around 20-35 min.\")\n",
        "      start_whisper = time.time()\n",
        "      return_code = subprocess.call(f\"whisper {mp3_file_path} --language en --model {whisper_model} --output_format json --model_dir {path_ml_models} --output_dir {path_split_mp3s} --verbose True\", shell=True)\n",
        "      logger.info(f\"Transcription has completed in {time.time()-start_whisper} seconds\")\n",
        "\n",
        "      logger.info(f\"Converting the output to template\")\n",
        "      # calling CLI whisper, it generates a json file that we will convert\n",
        "      # to the template\n",
        "      temp_json = glob.glob(path_split_mp3s + \"*.json\")[0]\n",
        "      data = json_load(temp_json)\n",
        "      transcript = data['text']\n",
        "      audio_segments = []\n",
        "      for row in data['segments']:\n",
        "        audio_segments.append({ \n",
        "            \"speaker\": \"SPEAKER\",\n",
        "            \"start\": row['start'], \n",
        "            \"stop\": row['end'], \n",
        "            \"duration\": row['end']-row['start'],\n",
        "            \"transcribed_text\": row[\"text\"]\n",
        "          })\n",
        "      json_save_to(audio_segments, file_path_audio_segments)\n",
        "      os.remove(temp_json)\n",
        "      json_save_to(transcript_get(audio_segments), file_path_transcript_path)\n",
        "      # converting audio_segments to the transcript\n",
        "      with open(file_path_transcript_path, 'w') as f:\n",
        "        f.write(transcript_get(audio_segments))\n",
        "      logger.info(f\"You can find this file at {file_path_transcript_path}\")\n",
        "    \n",
        "    if os.path.isfile(file_path_summary) == False and summarize_episode == True:\n",
        "      summary = summarize(transcript_wadsworth_constant(audio_segments))\n",
        "      logger.info(f\"Saving the summary at {file_path_summary}\")\n",
        "      with open(file_path_summary, 'w') as f:\n",
        "            f.write(summary)\n",
        "\n",
        "\n",
        "  dur= time.time() - entry_start\n",
        "  entry_duration.append(dur)\n",
        "  logger.info(f\"This episode took {dur} seconds\")\n",
        "\n",
        "  # update the podcast entries\n",
        "  feed_data['entries'][index] = entry\n",
        "  # update the file\n",
        "  json_save_to(feed_data, feed_json_path)\n",
        "  index += 1\n",
        "\n",
        "display(str(time.time()-start))\n",
        "# kill this session\n",
        "if auto_shutoff:\n",
        "  runtime.unassign()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NWHLGjRIMqfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (v3.9.13:6de2ca5339, May 17 2022, 11:37:23) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}